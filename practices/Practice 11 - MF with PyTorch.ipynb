{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2022/2023\n",
    "\n",
    "## Practice Session 11 - MF with PyTorch\n",
    "\n",
    "PyTorch, Tensorflow, Keras are useful framework that allow you to build machine learning models (from linear regression to complex deep learning methods) and hide almost all of the complexity related to the training. Usually, you only have to create an object that starting from the model parameters will be able to compute your prediction, then specify the loss and the framework automatically calculates the gradients.\n",
    "\n",
    "#### Performance warning!\n",
    "In image processing tasks one usually has an image, maybe a reasonably large one (1000x1000x3, hence 3\\*10^6 data points) on which a complex network is applied (multiple convolution operations, pooling etc). The computationally expensive part can be effectively parallelized by the framework and hence the speedup over using single-core operations is massive.\n",
    "\n",
    "Unfortunately here we are not dealing with images, but with user profiles. In terms of data this means each profile can be in the 10^5 - 10^6 items. The issue arises when considering the model. If you use a matrix factorization model, the core of the operation is a dot product between two embedding vectors, which is an extremely fast operation. There is hardly any speedup and the burden of the overall infrastructure is not offset by it. For this reason, if you use a profiler you will see that 80-90% of the time is spent in the data sampling phase (because it is done in python it can be quite slow) and the actual prediction computation is a tiny fraction of the time. Overall, a *simple* matrix factorization model may be 10x slower if implemented with pytorch. The gap will reduce if you use a powerful GPU but I am yet to find someone able to run that on a GPU faster than the Cython single-core implementation.\n",
    "\n",
    "#### Prototyping\n",
    "Given how the complexity of gradients and such is hidden, pytorch becomes a great tool for prototyping. It is very easy to change someting in your model becayse you do not need to dig in Cython code. For example, you may implement a SLIM MSE method that uses as inital parameters the similarity computed with an item-based cosine method, or you may create a hybrid of multiple similarities and lear the weights to use for each similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we need\n",
    "\n",
    "* A Dataset object to load the data\n",
    "* Model object\n",
    "* Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:25:53.715082Z",
     "start_time": "2023-11-25T20:25:50.856334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 597 (4.58 %) of 13025 users have no train items\n",
      "Warning: 2518 (19.33 %) of 13025 users have no sampled items\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sps\n",
    "\n",
    "from Data_manager.split_functions.split_train_validation_random_holdout import \\\n",
    "    split_train_in_two_percentage_global_sample\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "from Recommenders.KNN import ItemKNNCustomSimilarityRecommender\n",
    "from challenge.utils.functions import read_data, generate_submission_csv\n",
    "\n",
    "data_file_path = '../challenge/input_files/data_train.csv'\n",
    "users_file_path = '../challenge/input_files/data_target_users_test.csv'\n",
    "URM_all_dataframe, users_list = read_data(data_file_path, users_file_path)\n",
    "\n",
    "URM_all = sps.coo_matrix(\n",
    "    (URM_all_dataframe['Data'].values, (URM_all_dataframe['UserID'].values, URM_all_dataframe['ItemID'].values)))\n",
    "URM_all = URM_all.tocsr()\n",
    "\n",
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF models rely upon latent factors for users and items which are called 'embeddings'\n",
    "\n",
    "![latent factors](https://miro.medium.com/max/988/1*tiF4e4Y-wVH732_6TbJVmQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:25:57.637837Z",
     "start_time": "2023-11-25T20:25:57.621556Z"
    }
   },
   "outputs": [],
   "source": [
    "num_factors = 10\n",
    "\n",
    "n_users, n_items = URM_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:26:05.991075Z",
     "start_time": "2023-11-25T20:26:02.151385Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Creates U\n",
    "user_factors = torch.nn.Embedding(num_embeddings=n_users, embedding_dim=num_factors)\n",
    "\n",
    "# Creates V\n",
    "item_factors = torch.nn.Embedding(num_embeddings=n_items, embedding_dim=num_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:26:06.959444Z",
     "start_time": "2023-11-25T20:26:06.936193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(13025, 10)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:26:08.296232Z",
     "start_time": "2023-11-25T20:26:08.280156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(22348, 10)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to compute the prediction you have to:\n",
    "- Get a list of user and item indices (as tensors)\n",
    "- Get the user and item embedding\n",
    "- Compute the element-wise product of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:28:43.225536Z",
     "start_time": "2023-11-25T20:28:43.183235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([42]), tensor([42]))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_index = torch.Tensor([42]).type(torch.LongTensor)\n",
    "item_index = torch.Tensor([42]).type(torch.LongTensor)\n",
    "\n",
    "user_index, item_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that each object has a \"grad_fn=...\" attribute, which si going to be used for the automatic gradient compuation to go backwards in the operations required to compute the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:28:55.322760Z",
     "start_time": "2023-11-25T20:28:55.265491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 1.1120,  0.0877, -0.7391,  0.9084, -0.3965, -2.2140, -0.4592, -0.6728,\n          -0.7141,  0.8734]], grad_fn=<EmbeddingBackward0>),\n tensor([[-0.8987,  0.0286,  0.0557, -0.3426,  1.5130, -0.9608,  0.9584,  0.5459,\n          -0.0544,  1.0602]], grad_fn=<EmbeddingBackward0>))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_user_factors = user_factors(user_index)\n",
    "current_item_factors = item_factors(item_index)\n",
    "\n",
    "current_user_factors, current_item_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dot product is just a summation over the elementwise product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:29:03.172415Z",
     "start_time": "2023-11-25T20:29:03.138691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.3355, grad_fn=<SumBackward0>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = torch.mul(current_user_factors, current_item_factors).sum()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the \"grad_fn\" states \"SubBackward\", the prediction was indeed due to a sum\n",
    "\n",
    "#### We can also use the einstein summation format, which is particularly useful when you have a more complex equation to compute the prediction\n",
    "\n",
    "The einstein summation allows you to write the prediction in terms of the indices of a summation. In this case we want to iterate both embedding vectors, perform an element-by-element product and then sum at the end. Be careful on the dimensions, in this case the factors have two dimensions (the row dimension is 1 so in practice it is useless). We use \"b\" to iterate over the rows (useful when we compute batches of predictions to parallelize) and \"i\" is the latent factor index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:29:26.118936Z",
     "start_time": "2023-11-25T20:29:26.067908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.3355], grad_fn=<ViewBackward0>)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"bi,bi->b\", current_user_factors, current_item_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To take the result of the prediction and transform it into a traditional numpy array you have to:\n",
    "- call .detach() to disconnect the tensor from the automatic gradient tracking\n",
    "- then .numpy()\n",
    "\n",
    "### The result is an array of 1 cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:29:43.187607Z",
     "start_time": "2023-11-25T20:29:43.153853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is 0.34\n"
     ]
    }
   ],
   "source": [
    "prediction_numpy = prediction.detach().numpy()\n",
    "print(\"Prediction is {:.2f}\".format(prediction_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a MF MSE model with PyTorch\n",
    "\n",
    "# Step 1 Create a Model python object\n",
    "\n",
    "### The model should implement the forward function which computes the prediction as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:29:49.262854Z",
     "start_time": "2023-11-25T20:29:49.236687Z"
    }
   },
   "outputs": [],
   "source": [
    "class MF_MSE_PyTorch_model(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "        super(MF_MSE_PyTorch_model, self).__init__()\n",
    "\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "\n",
    "        self.user_factors = torch.nn.Embedding(num_embeddings=self.n_users, embedding_dim=n_factors)\n",
    "        self.item_factors = torch.nn.Embedding(num_embeddings=self.n_items, embedding_dim=n_factors)\n",
    "\n",
    "    def forward(self, user_batch, item_batch):\n",
    "        user_factors_batch = self.user_factors(user_batch)\n",
    "        item_factors_batch = self.item_factors(item_batch)\n",
    "\n",
    "        prediction_batch = torch.mul(user_factors_batch, item_factors_batch).sum()\n",
    "\n",
    "        return prediction_batch\n",
    "\n",
    "    def get_W(self):\n",
    "        return self.user_factors.weight.detach().cpu().numpy()\n",
    "\n",
    "    def get_H(self):\n",
    "        return self.item_factors.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Setup PyTorch devices and Data Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:29:52.902614Z",
     "start_time": "2023-11-25T20:29:52.742973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MF_MSE_PyTorch: Using CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"MF_MSE_PyTorch: Using CUDA\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"MF_MSE_PyTorch: Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the model and specify the device it should run on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:29:59.611277Z",
     "start_time": "2023-11-25T20:29:59.597438Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MF_MSE_PyTorch_model(n_users, n_items, num_factors).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose loss functions (Mean Squared Error in our case), there are quite a few to choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:30:02.217177Z",
     "start_time": "2023-11-25T20:30:02.187448Z"
    }
   },
   "outputs": [],
   "source": [
    "lossFunction = torch.nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively one can implement it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:30:07.008015Z",
     "start_time": "2023-11-25T20:30:06.976837Z"
    }
   },
   "outputs": [],
   "source": [
    "def _my_MSE_loss(model, user, item):\n",
    "    \n",
    "    # Compute prediction for each element in batch\n",
    "    prediction = model.forward(user, item)\n",
    "\n",
    "    # Compute total loss for batch\n",
    "    loss = (prediction - rating).pow(2).mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the optimizer to be used for the model parameters: Adam, AdaGrad, RMSProp etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:30:09.286413Z",
     "start_time": "2023-11-25T20:30:08.807441Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "l2_reg = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay = l2_reg*learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DatasetInteraction, which will be used to load a specific data point\n",
    "\n",
    "A DatasetInteraction will implement the Dataset class and provide the \\_\\_getitem\\_\\_(self, index) method, which allows to get the data points indexed by that index.\n",
    "\n",
    "Since we need the data to be a tensor, we pre inizialize everything as a tensor. In practice we save the URM in coordinate format (user, item, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:00.211856Z",
     "start_time": "2023-11-25T20:31:00.197245Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class DatasetInteraction(Dataset):\n",
    "    def __init__(self, URM):\n",
    "\n",
    "        URM = URM.tocoo()\n",
    "        self.n_data_points = URM.nnz\n",
    "\n",
    "        self._row = torch.tensor(URM.row).type(torch.LongTensor)\n",
    "        self._col = torch.tensor(URM.col).type(torch.LongTensor)\n",
    "        self._data = torch.tensor(URM.data).type(torch.FloatTensor)\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self._row[index], self._col[index], self._data[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We pass the DatasetIterator to a DataLoader object which manages the use of batches and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:01.935703Z",
     "start_time": "2023-11-25T20:31:01.907846Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# A large batch_size (256, 512...) improves parallelization, but the gradient becomes more smoot\n",
    "# at some point the performance will increase but at the expense of the final prediction quality\n",
    "batch_size = 64\n",
    "\n",
    "dataset_iterator = DatasetInteraction(URM_train)\n",
    "\n",
    "train_data_loader = DataLoader(dataset=dataset_iterator,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True,\n",
    "                               #num_workers = 2,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now we ran the usual epoch steps\n",
    "* Data point sampling\n",
    "* Prediction computation\n",
    "* Loss function computation\n",
    "* Gradient computation\n",
    "* Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:05.145345Z",
     "start_time": "2023-11-25T20:31:05.085734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([ 2424,  3784,  3405, 10030,  1253, 10183, 12704,   458,  9525,  9593,\n          2548,  2416, 10782,  4205, 10095,   623,  1765, 10112,  2277, 11945,\n          2902,  2520,  2123,  3839, 10759, 10276,  5187,  2239,  4301,  2054,\n          7767,  6565,  3764, 10642,  7016, 10394,  2039,  5933,  2540,  1670,\n          8778,  3233, 11981,  1107,  7510,  2953,  9544, 11030,  9637,  7977,\n         13020, 10942,  4575, 11890,  2591,  7722,  3956,  8262,  8467, 12476,\n          7290,  8467,  5193, 12626]),\n tensor([ 7316,  1128,  2988, 12441,   474, 13893,  4699,  2129, 15240, 18861,\n           315,    12,     7,     3,   760,  7266,   261, 14744,   220,  3696,\n          1091,   612,    62,  1914,  4491,    88,   141,  4439,   159,  3759,\n          1501,  1297,  6200,    59,   539, 16368,    50,  1567,  2387,  2046,\n          4482,  1417,    31,   112,  7957,    38,  2637, 21431,  3902,  1071,\n          1813, 18114,   849,   269,    59,  2600,   660,   272,  1947, 18501,\n          3443,   337,  2971,   572]),\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:20.364411Z",
     "start_time": "2023-11-25T20:31:06.892655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5985 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7ebe70871234776a5ca3fdf5cf00a87"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 s, sys: 3.96 s, total: 20.5 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "epoch_loss = 0\n",
    "for batch in tqdm(train_data_loader):\n",
    "\n",
    "    # Clear previously computed gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    user, item, rating = batch\n",
    "    \n",
    "    # Compute prediction for each element in batch\n",
    "    prediction = model.forward(user, item)\n",
    "    \n",
    "    # Compute total loss for batch\n",
    "    loss = (prediction - rating).pow(2).mean()\n",
    "\n",
    "    # Compute gradients given current loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Apply gradient using the selected optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the train is complete (it may take a while and many epochs), we can get the matrices in the usual numpy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:25.738709Z",
     "start_time": "2023-11-25T20:31:25.716710Z"
    }
   },
   "outputs": [],
   "source": [
    "user_factors = model.get_W()\n",
    "item_factors = model.get_H()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:26.808297Z",
     "start_time": "2023-11-25T20:31:26.591462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 0.42240247, -0.66577035, -0.6934146 , ..., -0.15180531,\n          1.2603949 ,  1.1509645 ],\n        [ 2.6244314 ,  0.33784166,  0.17933176, ..., -2.460579  ,\n          0.3136289 ,  0.50048804],\n        [ 0.43939874, -0.33758172,  2.323395  , ..., -0.11025701,\n         -2.0287244 ,  0.35721967],\n        ...,\n        [-0.62914175, -0.64939994, -0.38177714, ..., -0.7151549 ,\n         -0.9376109 ,  2.1543765 ],\n        [ 0.77411884,  1.6533009 ,  1.0868942 , ...,  1.1801665 ,\n          0.6116132 ,  1.1199809 ],\n        [ 0.16204336, -1.0171624 , -1.1031039 , ...,  0.40586352,\n          0.1465633 , -1.070071  ]], dtype=float32),\n (13025, 10))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors, user_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:27.518918Z",
     "start_time": "2023-11-25T20:31:27.477608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 2.1630244 , -0.03907879, -0.12325583, ..., -1.2066027 ,\n          0.7551582 ,  0.29103455],\n        [-1.0146065 ,  0.9341817 , -1.3799927 , ...,  1.2673644 ,\n         -1.5923662 , -0.46877384],\n        [-0.3062591 ,  0.4049404 , -0.5016469 , ...,  0.7301209 ,\n         -0.4796305 ,  0.94519335],\n        ...,\n        [ 1.3541249 ,  0.91673934, -0.00678596, ..., -1.3929248 ,\n          0.7228819 ,  0.654131  ],\n        [-1.487344  , -0.60572374,  1.8419907 , ...,  0.83821696,\n         -0.9122638 , -0.2620197 ],\n        [-0.6899875 ,  0.620203  ,  0.7397331 , ..., -0.373737  ,\n         -0.02342618, -0.7129373 ]], dtype=float32),\n (22348, 10))"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_factors, item_factors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if I want to change the sampling?\n",
    "The DatasetInteraction can be modified to obtain the desired behaviour, for example adding some negative (zero-rated) items in the sampling. If we want our model to be able to distinguish between positive and negative items we need to let the model see negative data as well, in our case the negative data is the zero-rated items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:43.417358Z",
     "start_time": "2023-11-25T20:31:43.393777Z"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetInteraction(Dataset):\n",
    "    def __init__(self, URM_train, positive_quota):\n",
    "        \n",
    "        self._URM_train = sps.csr_matrix(URM_train)\n",
    "        \n",
    "        URM_train = URM_train.tocoo()\n",
    "        self.n_data_points = URM_train.nnz\n",
    "\n",
    "        self._row = torch.tensor(URM_train.row).type(torch.LongTensor)\n",
    "        self._col = torch.tensor(URM_train.col).type(torch.LongTensor)\n",
    "        self._data = torch.tensor(URM_train.data).type(torch.FloatTensor)\n",
    "        self._positive_quota = positive_quota\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        select_positive_flag = torch.rand(1, requires_grad=False) > self._positive_quota\n",
    "\n",
    "        if select_positive_flag[0]:\n",
    "            return self._row[index], self._col[index], self._data[index]\n",
    "        else:\n",
    "            user_id = self._row[index]\n",
    "            seen_items = self._URM_train.indices[self._URM_train.indptr[user_id]:self._URM_train.indptr[user_id+1]]\n",
    "            negative_selected = False\n",
    "\n",
    "            while not negative_selected:\n",
    "                negative_candidate = torch.randint(low=0, high=self.n_items, size=(1,))[0]\n",
    "\n",
    "                if negative_candidate not in seen_items:\n",
    "                    item_negative = negative_candidate\n",
    "                    negative_selected = True\n",
    "\n",
    "            return self._row[index], item_negative, torch.tensor(0.0)\n",
    "\n",
    "        \n",
    "        return self._row[index], self._col[index], self._data[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also change the dataset iterator to one that samples the user profile rather than the specific interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:44.889231Z",
     "start_time": "2023-11-25T20:31:44.876693Z"
    }
   },
   "outputs": [],
   "source": [
    "class UserProfile_Dataset(Dataset):\n",
    "    def __init__(self, URM_train, device):\n",
    "        super().__init__()\n",
    "        URM_train = sps.csr_matrix(URM_train)\n",
    "        self.device = device\n",
    "\n",
    "        self.n_users, self.n_items = URM_train.shape\n",
    "        self._indptr = URM_train.indptr\n",
    "        self._indices = torch.tensor(URM_train.indices, dtype = torch.long, device=device)\n",
    "        self._data = torch.tensor(URM_train.data, dtype = torch.float, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_users\n",
    "\n",
    "    def __getitem__(self, user_id):\n",
    "        start_pos = self._indptr[user_id]\n",
    "        end_pos = self._indptr[user_id+1]\n",
    "\n",
    "        user_profile = torch.zeros(self.n_items, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        user_profile[self._indices[start_pos:end_pos]] = self._data[start_pos:end_pos]\n",
    "\n",
    "        return user_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if I want to implement AsySVD? SLIM EN ... \n",
    "You just have to change the pytorch model with the desired one (easy to do)\n",
    "\n",
    "Note these two models work by sampling the whole user profile, they can be adapted to a sampler that provides single interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:51.566901Z",
     "start_time": "2023-11-25T20:31:51.548823Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AsySVDModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size = None, n_items = None, device = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self._embedding_item_1 = torch.nn.Parameter(torch.randn((n_items, embedding_size)))\n",
    "        self._embedding_item_2 = torch.nn.Parameter(torch.randn((embedding_size, n_items)))\n",
    "\n",
    "    def forward(self, user_profile_batch):\n",
    "        # input shape is batch_size x n items\n",
    "        # r_hat_bi = SUM{e=0}{e=embedding_size} SUM{j=0}{j=n items} r_bj * V1_je * V2_ei\n",
    "        layer_output = torch.einsum(\"bj,je,ei->bi\", user_profile_batch, self._embedding_item_1, self._embedding_item_2)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:52.287178Z",
     "start_time": "2023-11-25T20:31:52.272112Z"
    }
   },
   "outputs": [],
   "source": [
    "class SDenseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_items = None, device = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self._S = torch.nn.Parameter(torch.zeros((n_items, n_items)))\n",
    "\n",
    "    def forward(self, user_profile_batch):\n",
    "        # input shape is batch_size x n items\n",
    "        # r_hat_bi = SUM{j=0}{j=n items} r_bj * S_ji\n",
    "        layer_output = torch.einsum(\"bj,ji->bi\", user_profile_batch, self._S)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if I want to change the loss function?\n",
    "You can just implement the new one, BPR is quite simple. Make sure that the dataset iterator samples the right data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:56.888153Z",
     "start_time": "2023-11-25T20:31:56.870078Z"
    }
   },
   "outputs": [],
   "source": [
    "class BPR_Dataset(Dataset):\n",
    "    def __init__(self, URM_train):\n",
    "        super().__init__()\n",
    "        self._URM_train = sps.csr_matrix(URM_train)\n",
    "        self.n_users, self.n_items = self._URM_train.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_users\n",
    "\n",
    "    def __getitem__(self, user_id):\n",
    "\n",
    "        seen_items = self._URM_train.indices[self._URM_train.indptr[user_id]:self._URM_train.indptr[user_id+1]]\n",
    "        item_positive = np.random.choice(seen_items)\n",
    "\n",
    "        negative_selected = False\n",
    "\n",
    "        while not negative_selected:\n",
    "            negative_candidate = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "\n",
    "            if negative_candidate not in seen_items:\n",
    "                item_negative = negative_candidate\n",
    "                negative_selected = True\n",
    "\n",
    "        return user_id, item_positive, item_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T20:31:57.817903Z",
     "start_time": "2023-11-25T20:31:57.798915Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_BPR(model, batch):\n",
    "    user, item_positive, item_negative = batch\n",
    "\n",
    "    # Compute prediction for each element in batch\n",
    "    x_ij = model.forward(user, item_positive) - model.forward(user, item_negative)\n",
    "\n",
    "    # Compute total loss for batch\n",
    "    loss = -x_ij.sigmoid().log().mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
