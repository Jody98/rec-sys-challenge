{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2021/22\n",
    "\n",
    "### Practice - Implicit Alternating Least Squares\n",
    "\n",
    "See:\n",
    "Y. Hu, Y. Koren and C. Volinsky, Collaborative filtering for implicit feedback datasets, ICDM 2008.\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.5120&rep=rep1&type=pdf\n",
    "\n",
    "R. Pan et al., One-class collaborative filtering, ICDM 2008.\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.306.4684&rep=rep1&type=pdf\n",
    "\n",
    "Factorization model for binary feedback.\n",
    "First, splits the feedback matrix R as the element-wise a Preference matrix P and a Confidence matrix C.\n",
    "Then computes the decomposition of them into the dot product of two matrices X and Y of latent factors.\n",
    "X represent the user latent factors, Y the item latent factors.\n",
    "\n",
    "The model is learned by solving the following regularized Least-squares objective function with Stochastic Gradient Descent\n",
    "    \n",
    "$$\\frac{1}{2}\\sum_{i,j}{c_{ij}\\left(p_{ij}-x_i^T y_j\\right) + \\lambda\\left(\\sum_{i}{||x_i||^2} + \\sum_{j}{||y_j||^2}\\right)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T13:54:08.861051Z",
     "start_time": "2023-11-19T13:54:08.741281Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T13:54:43.486169Z",
     "start_time": "2023-11-19T13:54:39.652871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 600 (4.61 %) of 13025 users have no train items\n",
      "Warning: 2580 (19.81 %) of 13025 users have no sampled items\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sps\n",
    "\n",
    "from Data_manager.split_functions.split_train_validation_random_holdout import \\\n",
    "    split_train_in_two_percentage_global_sample\n",
    "from challenge.utils.functions import read_data, evaluate_algorithm, generate_submission_csv\n",
    "\n",
    "\n",
    "data_file_path = '../challenge/input_files/data_train.csv'\n",
    "users_file_path = '../challenge/input_files/data_target_users_test.csv'\n",
    "URM_all_dataframe, users_list = read_data(data_file_path, users_file_path)\n",
    "\n",
    "URM_all = sps.coo_matrix(\n",
    "    (URM_all_dataframe['Data'].values, (URM_all_dataframe['UserID'].values, URM_all_dataframe['ItemID'].values)))\n",
    "URM_all = URM_all.tocsr()\n",
    "\n",
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T13:54:56.075657Z",
     "start_time": "2023-11-19T13:54:56.048745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<13025x22348 sparse matrix of type '<class 'numpy.float64'>'\n\twith 382984 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we need for IALS?\n",
    "\n",
    "* User factor and Item factor matrices\n",
    "* Confidence function\n",
    "* Update rule for items\n",
    "* Update rule for users\n",
    "* Training loop and some patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T13:55:02.633295Z",
     "start_time": "2023-11-19T13:55:02.609489Z"
    }
   },
   "outputs": [],
   "source": [
    "n_users, n_items = URM_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: We create the dense latent factor matrices\n",
    "### In a MF model you have two matrices, one with a row per user and the other with a column per item. The other dimension, columns for the first one and rows for the second one is called latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T13:55:10.304924Z",
     "start_time": "2023-11-19T13:55:10.289155Z"
    }
   },
   "outputs": [],
   "source": [
    "num_factors = 10\n",
    "\n",
    "user_factors = np.random.random((n_users, num_factors))\n",
    "item_factors = np.random.random((n_items, num_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T13:55:11.388031Z",
     "start_time": "2023-11-19T13:55:11.372378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.16407308, 0.00649691, 0.6579195 , ..., 0.88932852, 0.38336941,\n        0.24233853],\n       [0.25380496, 0.78139655, 0.47604554, ..., 0.79615149, 0.70535599,\n        0.38206734],\n       [0.76390603, 0.5622817 , 0.47190413, ..., 0.30212421, 0.88263303,\n        0.37869519],\n       ...,\n       [0.4539675 , 0.93940015, 0.91809905, ..., 0.96307741, 0.70820284,\n        0.60168815],\n       [0.4321185 , 0.27190809, 0.82540688, ..., 0.90222375, 0.84766443,\n        0.53892239],\n       [0.89662591, 0.4515939 , 0.77216422, ..., 0.35550742, 0.35134464,\n        0.93892266]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T13:55:12.617117Z",
     "start_time": "2023-11-19T13:55:12.583871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.46353391, 0.59425837, 0.09775851, ..., 0.58334571, 0.06496289,\n        0.04005821],\n       [0.08607168, 0.12711198, 0.24479113, ..., 0.90519102, 0.01483432,\n        0.37950819],\n       [0.28029474, 0.64324866, 0.98812874, ..., 0.44800259, 0.45167813,\n        0.15554027],\n       ...,\n       [0.54313751, 0.62692431, 0.3285164 , ..., 0.41258404, 0.02190922,\n        0.78755353],\n       [0.21891638, 0.23277344, 0.18059221, ..., 0.79882938, 0.25701765,\n        0.63449884],\n       [0.30587698, 0.70257865, 0.38267958, ..., 0.80949176, 0.78436452,\n        0.55857392]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: We define a function to transform the interaction data in a \"confidence\" value. \n",
    "* If you have explicit data, the higher it is the higher the confidence (logarithmic, linear?)\n",
    "* Other options include scaling the data lowering it if the item or use has very few interactions (lower support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T13:55:37.339231Z",
     "start_time": "2023-11-19T13:55:37.321533Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_confidence_function(URM_train, alpha):\n",
    "    \n",
    "    URM_train.data = 1.0 + alpha*URM_train.data\n",
    "    \n",
    "    return URM_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T13:55:38.015923Z",
     "start_time": "2023-11-19T13:55:37.949011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "C_URM_train = linear_confidence_function(URM_train, alpha)\n",
    "\n",
    "C_URM_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of confidence can be defined in different ways, for example in terms of the number of interactions an item or a user has, the more they have the more support your model will have for the respective latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T13:56:05.632127Z",
     "start_time": "2023-11-19T13:56:05.615861Z"
    }
   },
   "outputs": [],
   "source": [
    "def popularity_confidence(URM_train):\n",
    "    \n",
    "    item_popularity = np.ediff1d(URM_train.tocsc().indptr)\n",
    "    item_confidence = np.zeros(len(item_popularity))\n",
    "    item_confidence[item_popularity!=0] = np.log(item_popularity[item_popularity!=0])\n",
    "    \n",
    "    C_URM_train = URM_train.copy()\n",
    "    C_URM_train = C_URM_train.tocsc()\n",
    "    \n",
    "    for item_id in range(C_URM_train.shape[1]):\n",
    "        start_pos = C_URM_train.indptr[item_id]\n",
    "        end_pos = C_URM_train.indptr[item_id+1]\n",
    "        \n",
    "        C_URM_train.data[start_pos:end_pos] = item_confidence[item_id]\n",
    "    \n",
    "    C_URM_train = C_URM_train.tocsr()\n",
    "    \n",
    "    return C_URM_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T13:56:07.195477Z",
     "start_time": "2023-11-19T13:56:07.152513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([7.01750614, 6.69703425, 5.78382518, 5.26269019, 5.33271879,\n       5.39362755, 5.36129217, 5.50125821, 5.06259503, 4.93447393])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_URM_train = popularity_confidence(URM_train)\n",
    "\n",
    "C_URM_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the update rules for the user factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Update latent factors for a single user or item.\n",
    "\n",
    "Y = |n_interactions|x|n_factors|\n",
    "\n",
    "YtY =   |n_factors|x|n_factors|\n",
    "\n",
    "\n",
    "\n",
    "Latent factors ony of item/users for which an interaction exists in the interaction profile\n",
    "Y_interactions = Y[interaction_profile, :]\n",
    "\n",
    "Following the notation of the original paper we report the update rule for the Item factors (User factors are identical):\n",
    "* __Y__ are the item factors |n_items|x|n_factors|\n",
    "* __Cu__ is a diagonal matrix |n_interactions|x|n_interactions| with the user confidence for the observed items\n",
    "* __p(u)__ is a boolean vectors indexing only observed items. Here it will disappear as we already extract only the observed latent factors however, it will have an impact in the dimensions of the matrix, since it transforms Cu from a diagonal matrix to a row vector of 1 row and |n_interactions| columns\n",
    "\n",
    "$$(Yt*Cu*Y + reg*I)^-1 * Yt*Cu*profile$$ which can be decomposed as $$(YtY + Yt*(Cu-I)*Y + reg*I)^-1 * Yt*Cu*p(u)$$ \n",
    "\n",
    "* __A__ = (|n_interactions|x|n_factors|) dot (|n_interactions|x|n_interactions| ) dot (|n_interactions|x|n_factors| )\n",
    "  = |n_factors|x|n_factors|\n",
    "  \n",
    "We use an equivalent formulation (v * k.T).T which is much faster\n",
    "* __A__ = Y_interactions.T.dot(((interaction_confidence - 1) * Y_interactions.T).T)\n",
    "* __B__ = YtY + A + self.regularization_diagonal\n",
    "* __new factors__ = np.dot(np.linalg.inv(B), Y_interactions.T.dot(interaction_confidence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:01:14.094378Z",
     "start_time": "2023-11-19T14:01:14.077651Z"
    }
   },
   "outputs": [],
   "source": [
    "def _update_row(interaction_profile, interaction_confidence, Y, YtY, regularization_diagonal):\n",
    "\n",
    "    Y_interactions = Y[interaction_profile, :]\n",
    "    \n",
    "    A = Y_interactions.T.dot(((interaction_confidence - 1) * Y_interactions.T).T)\n",
    "\n",
    "    B = YtY + A + regularization_diagonal\n",
    "\n",
    "    return np.dot(np.linalg.inv(B), Y_interactions.T.dot(interaction_confidence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:01:17.594853Z",
     "start_time": "2023-11-19T14:01:17.578307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.0001, 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n        0.    , 0.    ],\n       [0.    , 0.0001, 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n        0.    , 0.    ],\n       [0.    , 0.    , 0.0001, 0.    , 0.    , 0.    , 0.    , 0.    ,\n        0.    , 0.    ],\n       [0.    , 0.    , 0.    , 0.0001, 0.    , 0.    , 0.    , 0.    ,\n        0.    , 0.    ],\n       [0.    , 0.    , 0.    , 0.    , 0.0001, 0.    , 0.    , 0.    ,\n        0.    , 0.    ],\n       [0.    , 0.    , 0.    , 0.    , 0.    , 0.0001, 0.    , 0.    ,\n        0.    , 0.    ],\n       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.0001, 0.    ,\n        0.    , 0.    ],\n       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.0001,\n        0.    , 0.    ],\n       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n        0.0001, 0.    ],\n       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n        0.    , 0.0001]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularization_coefficient = 1e-4\n",
    "\n",
    "regularization_diagonal = np.diag(regularization_coefficient * np.ones(num_factors))\n",
    "regularization_diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:01:20.523354Z",
     "start_time": "2023-11-19T14:01:20.504965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 10)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VV = n_factors x n_factors\n",
    "VV = item_factors.T.dot(item_factors)\n",
    "VV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:01:25.102573Z",
     "start_time": "2023-11-19T14:01:25.082808Z"
    }
   },
   "outputs": [],
   "source": [
    "user_id = 154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:01:26.361650Z",
     "start_time": "2023-11-19T14:01:26.337122Z"
    }
   },
   "outputs": [],
   "source": [
    "C_URM_train = linear_confidence_function(URM_train, alpha)\n",
    "\n",
    "start_pos = C_URM_train.indptr[user_id]\n",
    "end_pos = C_URM_train.indptr[user_id + 1]\n",
    "\n",
    "user_profile = C_URM_train.indices[start_pos:end_pos]\n",
    "user_confidence = C_URM_train.data[start_pos:end_pos]\n",
    "\n",
    "user_factors[user_id, :] = _update_row(user_profile, user_confidence, item_factors, VV, regularization_diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply updates on the user item factors as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:01:30.072664Z",
     "start_time": "2023-11-19T14:01:30.056923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 10)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UU = n_factors x n_factors\n",
    "UU = user_factors.T.dot(user_factors)\n",
    "UU.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:01:38.683074Z",
     "start_time": "2023-11-19T14:01:38.663629Z"
    }
   },
   "outputs": [],
   "source": [
    "item_id = 154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:01:39.614394Z",
     "start_time": "2023-11-19T14:01:39.589127Z"
    }
   },
   "outputs": [],
   "source": [
    "C_URM_train_csc = C_URM_train.tocsc()\n",
    "\n",
    "start_pos = C_URM_train_csc.indptr[item_id]\n",
    "end_pos = C_URM_train_csc.indptr[item_id + 1]\n",
    "\n",
    "item_profile = C_URM_train_csc.indices[start_pos:end_pos]\n",
    "item_confidence = C_URM_train_csc.data[start_pos:end_pos]\n",
    "\n",
    "item_factors[item_id, :] = _update_row(item_profile, item_confidence, user_factors, UU, regularization_diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put all together in a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:01:58.607376Z",
     "start_time": "2023-11-19T14:01:42.935703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13025 in 0.51 seconds. Users per second 25345.88\n",
      "Iteration 22348 in 1.35 seconds. Items per second 16509.03\n",
      "Epoch 1 complete in in 1.35 seconds\n",
      "Iteration 13025 in 0.52 seconds. Users per second 24857.77\n",
      "Iteration 22348 in 1.38 seconds. Items per second 16155.71\n",
      "Epoch 2 complete in in 1.38 seconds\n",
      "Iteration 13025 in 0.57 seconds. Users per second 22928.73\n",
      "Iteration 22348 in 1.61 seconds. Items per second 13864.97\n",
      "Epoch 3 complete in in 1.61 seconds\n",
      "Iteration 13025 in 0.64 seconds. Users per second 20352.45\n",
      "Iteration 22348 in 1.69 seconds. Items per second 13219.90\n",
      "Epoch 4 complete in in 1.69 seconds\n",
      "Iteration 13025 in 0.63 seconds. Users per second 20605.12\n",
      "Iteration 22348 in 1.63 seconds. Items per second 13708.34\n",
      "Epoch 5 complete in in 1.63 seconds\n",
      "Iteration 13025 in 0.59 seconds. Users per second 22076.34\n",
      "Iteration 22348 in 1.56 seconds. Items per second 14345.86\n",
      "Epoch 6 complete in in 1.56 seconds\n",
      "Iteration 13025 in 0.58 seconds. Users per second 22331.39\n",
      "Iteration 22348 in 1.58 seconds. Items per second 14182.72\n",
      "Epoch 7 complete in in 1.58 seconds\n",
      "Iteration 13025 in 0.63 seconds. Users per second 20689.67\n",
      "Iteration 22348 in 1.64 seconds. Items per second 13642.38\n",
      "Epoch 8 complete in in 1.64 seconds\n",
      "Iteration 13025 in 0.62 seconds. Users per second 20914.49\n",
      "Iteration 22348 in 1.63 seconds. Items per second 13734.82\n",
      "Epoch 9 complete in in 1.63 seconds\n",
      "Iteration 13025 in 0.61 seconds. Users per second 21519.77\n",
      "Iteration 22348 in 1.59 seconds. Items per second 14058.02\n",
      "Epoch 10 complete in in 1.59 seconds\n"
     ]
    }
   ],
   "source": [
    "C_URM_train_csc = C_URM_train.tocsc()\n",
    "\n",
    "num_factors = 10\n",
    "\n",
    "user_factors = np.random.random((n_users, num_factors))\n",
    "item_factors = np.random.random((n_items, num_factors))\n",
    "\n",
    "\n",
    "for n_epoch in range(10):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    VV = item_factors.T.dot(item_factors)\n",
    "        \n",
    "    for user_id in range(C_URM_train.shape[0]):\n",
    "\n",
    "        start_pos = C_URM_train.indptr[user_id]\n",
    "        end_pos = C_URM_train.indptr[user_id + 1]\n",
    "\n",
    "        user_profile = C_URM_train.indices[start_pos:end_pos]\n",
    "        user_confidence = C_URM_train.data[start_pos:end_pos]\n",
    "        \n",
    "        user_factors[user_id, :] = _update_row(user_profile, user_confidence, item_factors, VV, regularization_diagonal)   \n",
    "\n",
    "        # Print some stats\n",
    "        if (user_id +1)% 100000 == 0 or user_id == C_URM_train.shape[0]-1:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            samples_per_second = user_id/elapsed_time\n",
    "            print(\"Iteration {} in {:.2f} seconds. Users per second {:.2f}\".format(user_id+1, elapsed_time, samples_per_second))\n",
    "    \n",
    "    UU = user_factors.T.dot(user_factors)\n",
    "\n",
    "    for item_id in range(C_URM_train.shape[1]):\n",
    "\n",
    "        start_pos = C_URM_train_csc.indptr[item_id]\n",
    "        end_pos = C_URM_train_csc.indptr[item_id + 1]\n",
    "\n",
    "        item_profile = C_URM_train_csc.indices[start_pos:end_pos]\n",
    "        item_confidence = C_URM_train_csc.data[start_pos:end_pos]\n",
    "\n",
    "        item_factors[item_id, :] = _update_row(item_profile, item_confidence, user_factors, UU, regularization_diagonal)    \n",
    "\n",
    "        # Print some stats\n",
    "        if (item_id +1)% 100000 == 0 or item_id == C_URM_train.shape[1]-1:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            samples_per_second = item_id/elapsed_time\n",
    "            print(\"Iteration {} in {:.2f} seconds. Items per second {:.2f}\".format(item_id+1, elapsed_time, samples_per_second))\n",
    "\n",
    "    total_epoch_time = time.time() - start_time  \n",
    "    print(\"Epoch {} complete in in {:.2f} seconds\".format(n_epoch+1, total_epoch_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long do we train such a model?\n",
    "\n",
    "* An epoch: a complete loop over all the train data\n",
    "* Usually you train for multiple epochs. Depending on the algorithm and data 10s or 100s of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:02:28.603040Z",
     "start_time": "2023-11-19T14:02:28.578823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time with the previous training speed is 15.90 seconds, or 0.26 minutes\n"
     ]
    }
   ],
   "source": [
    "estimated_seconds = total_epoch_time*10\n",
    "print(\"Estimated time with the previous training speed is {:.2f} seconds, or {:.2f} minutes\".format(estimated_seconds, estimated_seconds/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lastly: Computing a prediction for any given user or item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:05:28.663533Z",
     "start_time": "2023-11-19T14:05:28.645644Z"
    }
   },
   "outputs": [],
   "source": [
    "user_id = 13024\n",
    "item_id = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T14:05:28.972172Z",
     "start_time": "2023-11-19T14:05:28.957676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.11505826723711587"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_rating = np.dot(user_factors[user_id,:], item_factors[item_id,:])\n",
    "predicted_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
